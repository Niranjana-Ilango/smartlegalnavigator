# -*- coding: utf-8 -*-
"""Smart Legal Navigator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QCF9Q7jqBYVD7nEm3ATP33LDxJL6fw7
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/SmartLegalNavigator/data

from google.colab import files
uploaded = files.upload()

import shutil

shutil.move('constitution_qa.json', '/content/drive/MyDrive/SmartLegalNavigator/data/constitution_qa.json')
shutil.move('ipc_qa.json', '/content/drive/MyDrive/SmartLegalNavigator/data/ipc_qa.json')
shutil.move('crpc_qa.json', '/content/drive/MyDrive/SmartLegalNavigator/data/crpc_qa.json')
shutil.move('reduced_combined_qa.json', '/content/drive/MyDrive/SmartLegalNavigator/data/reduced_combined_qa.json')

import json

def load_qa_data():
    base_path = '/content/drive/MyDrive/SmartLegalNavigator/data/'
    files = ['constitution_qa.json', 'ipc_qa.json', 'crpc_qa.json', 'reduced_combined_qa.json']
    context_list = []
    for file in files:
        with open(base_path + file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])
    return " ".join(context_list)

legal_context = load_qa_data()

def generate_audio(text, lang_code):
    tts = gTTS(text=text, lang=lang_code)
    path = f"/content/drive/MyDrive/SmartLegalNavigator/audio/audio_{random.randint(0,9999)}.mp3"
    tts.save(path)
    return path

from transformers import pipeline

qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

translator = Translator()

def smart_legal_assist(user_input, lang_code):
    if lang_code != "en":
        translated_input = translator.translate(user_input, src=lang_code, dest='en').text
    else:
        translated_input = user_input

    result = qa_pipeline(question=translated_input, context=legal_context)
    answer = result['answer']

    if lang_code != "en":
        translated_answer = translator.translate(answer, src='en', dest=lang_code).text
    else:
        translated_answer = answer

    return translated_answer

pip install gtts

from gtts import gTTS
import random

def generate_audio(text, lang_code):
    tts = gTTS(text=text, lang=lang_code)
    file_path = f"audio_{random.randint(0,10000)}.mp3"
    tts.save(file_path)
    return file_path

!pip install SpeechRecognition

import speech_recognition as sr

def process_voice(audio, lang_code):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio) as source:
        audio_data = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio_data, language=lang_code)
            return text
        except:
            return "Sorry, I couldn't understand the audio."

def handle_input(text, audio, lang_choice):
    lang_map = {
        "English": "en",
        "Tamil": "ta",
        "Hindi": "hi",
        "Telugu": "te",
        "Malayalam": "ml",
        "Kannada": "kn"
    }

    lang_code = lang_map.get(lang_choice, "en")

    if audio is not None:
        user_query = process_voice(audio, lang_code)
    else:
        user_query = text

    if user_query.strip() == "":
        return "Please provide a question.", None, None

    response = smart_legal_assist(user_query, lang_code)
    audio_path = generate_audio(response, lang_code)

    return user_query, response, audio_path

!pip install gradio

def handle_input(text, audio, lang_choice):
    lang_map = {
        "English": "en",
        "Tamil": "ta",
        "Hindi": "hi",
        "Telugu": "te",
        "Malayalam": "ml",
        "Kannada": "kn"
    }

    lang_code = lang_map.get(lang_choice, "en")

    if audio is not None:
        user_query = process_voice(audio, lang_code)
    else:
        user_query = text

    if user_query.strip() == "":
        return "Please provide a question.", None, None

    response = smart_legal_assist(user_query, lang_code)
    audio_path = generate_audio(response, lang_code)

    return user_query, response, audio_path

def handle_input(text, audio, lang_choice):
    try:
        lang_map = {
            "English": "en",
            "Tamil": "ta",
            "Hindi": "hi",
            "Telugu": "te",
            "Malayalam": "ml",
            "Kannada": "kn"
        }

        lang_code = lang_map.get(lang_choice, "en")

        # Choose audio or text input
        if audio is not None:
            user_query = process_voice(audio, lang_code)
        else:
            user_query = text

        if not user_query.strip():
            return "No question provided.", "N/A", None

        # Get response
        response = smart_legal_assist(user_query, lang_code)

        # Convert response to audio
        audio_path = generate_audio(response, lang_code)

        return user_query, response, audio_path

    except Exception as e:
        # Show exact error in output
        return f"Error: {str(e)}", "Error", None

!pip install transformers gradio gtts SpeechRecognition

file_paths = [
    '/content/drive/MyDrive/ipc.json',
    '/content/drive/MyDrive/crpc.json',
    '/content/drive/MyDrive/constitution.json'
]

def load_legal_data():
    return [
        "Article 1: India, that is Bharat, shall be a Union of States.",
        "Section 302 IPC: Whoever commits murder shall be punished with death or imprisonment for life.",
        "Section 124A IPC: Sedition ‚Äì whoever by words or signs brings hatred against the government."
    ]

# Install necessary packages (only run once)
!pip install transformers scikit-learn

from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os

# Step 1: Mount Google Drive (only in Colab)
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Load legal data
def load_legal_data():
    context_list = []
    file_paths = [
        '/content/ipc_qa (1).json',
        '/content/crpc_qa (1).json',
        '/content/constitution_qa (1).json'
    ]

    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                for entry in data:
                    context_list.append(entry.get('answer', ''))
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è Could not parse JSON from {path}")

    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")

    return context_list

# Step 3: Load model and data
legal_context = load_legal_data()
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Step 4: Answer function
def smart_legal_assist(user_question, lang_code='en'):
    if not user_question:
        return "‚ö†Ô∏è Please enter a legal question."

    # Find the most relevant answer using cosine similarity
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()

    if similarities.max() == 0:
        return "ü§∑ Sorry, I couldn't find any relevant information."

    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]

    # Extract answer using QA model
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

import gradio as gr

lang_options = ["English", "Tamil", "Hindi", "Telugu", "Malayalam", "Kannada"]

interface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question here (optional)"),
        gr.Audio(type="filepath", label="Or upload a voice question"),
        gr.Dropdown(choices=lang_options, value="English", label="Select Language")
    ],
    outputs=[
        gr.Textbox(label="Recognized Question"),
        gr.Textbox(label="Legal Answer"),
        gr.Audio(label="Answer in Voice")
    ],
    title=" Smart Legal Navigator",
    description="Ask legal questions. Multilingual voice + text input."
)

interface.launch()

import gradio as gr                           #text alone working
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os

# Load legal context from files
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json"
    ]
    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])
    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Load data and models
legal_context = load_legal_data()
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Smart Legal Assist function
def smart_legal_assist(user_question):
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

# Gradio interface
def gradio_interface(user_question):
    try:
        return smart_legal_assist(user_question)
    except Exception as e:
        return f"Error: {str(e)}"

gr.Interface(
    fn=gradio_interface,
    inputs=gr.Textbox(lines=2, placeholder="Enter your legal question..."),
    outputs="text",
    title="Smart Legal Navigator",
    description="Ask legal questions based on Indian legal documents."
).launch()

!pip install transformers sentencepiece

import gradio as gr           #text multilingual working
from transformers import pipeline, M2M100ForConditionalGeneration, M2M100Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os

# Load translation model
model_name = "facebook/m2m100_418M"
translator_tokenizer = M2M100Tokenizer.from_pretrained(model_name)
translator_model = M2M100ForConditionalGeneration.from_pretrained(model_name)

def translate(text, source_lang, target_lang):
    translator_tokenizer.src_lang = source_lang
    encoded = translator_tokenizer(text, return_tensors="pt")
    generated = translator_model.generate(**encoded, forced_bos_token_id=translator_tokenizer.get_lang_id(target_lang))
    return translator_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]

# Load legal data
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json"
    ]
    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])
    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

legal_context = load_legal_data()
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Main legal QA function with multilingual support
def smart_legal_assist(user_question, language):
    lang_codes = {
        "English": "en",
        "Hindi": "hi",
        "Tamil": "ta",
        "Telugu": "te",
        "Bengali": "bn"
    }
    source_lang = lang_codes.get(language, "en")

    # Translate question to English
    if source_lang != "en":
        user_question = translate(user_question, source_lang, "en")

    # Run QA
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    answer = result['answer']

    # Translate answer back
    if source_lang != "en":
        answer = translate(answer, "en", source_lang)
    return answer

# Gradio UI
def gradio_interface(user_question, language):
    try:
        return smart_legal_assist(user_question, language)
    except Exception as e:
        return f"Error: {str(e)}"

gr.Interface(
    fn=gradio_interface,
    inputs=[
        gr.Textbox(lines=2, label="Enter your legal question"),
        gr.Dropdown(choices=["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language")
    ],
    outputs="text",
    title="Smart Legal Navigator (Multilingual)",
    description="Ask legal questions in English, Hindi, Tamil, Telugu, or Bengali."
).launch()

!pip install deep-translator

import gradio as gr     # not as apt to answer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
import torch
import tempfile
from gtts import gTTS
from deep_translator import GoogleTranslator

# Load legal data
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/reduced_combined_qa.json"
    ]

    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])

    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Set up model
device = 0 if torch.cuda.is_available() else -1
print(f"Device set to use {'GPU' if device == 0 else 'CPU'}")
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad", device=device)

# Load and vectorize data
legal_context = load_legal_data()
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Smart legal assistant logic
def smart_legal_assist(user_question):
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

# Actual Translation Logic
def translate_text(text, src_lang, tgt_lang):
    if src_lang == tgt_lang or not text:
        return text
    try:
        translated = GoogleTranslator(source=src_lang.lower(), target=tgt_lang.lower()).translate(text)
        return translated
    except Exception as e:
        return f"Translation error: {str(e)}"

# Voice + Text processing
def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    # Voice input if no text
    if not input_text and audio_input:
        import speech_recognition as sr
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_input) as source:
            audio = recognizer.record(source)
        try:
            input_text = recognizer.recognize_google(audio)
        except sr.UnknownValueError:
            return "Sorry, I could not understand the audio.", None

    if not input_text:
        return "Please enter or say a legal question.", None

    # Get answer in English
    answer_en = smart_legal_assist(input_text)

    # Translate to selected language
    lang_map = {
        "English": "en",
        "Hindi": "hi",
        "Tamil": "ta",
        "Telugu": "te",
        "Bengali": "bn"
    }

    lang_code = lang_map.get(lang_choice, "en")
    final_answer = translate_text(answer_en, "en", lang_code)

    # Convert to voice
    tts = gTTS(text=final_answer, lang=lang_code)
    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_audio.name)

    return final_answer, temp_audio.name

# Gradio UI
iface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question (optional)"),
        gr.Audio(type="filepath", label="Or ask your question by voice"),
        gr.Dropdown(["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language")
    ],
    outputs=[
        gr.Textbox(label="Legal Answer"),
        gr.Audio(label="Answer in Voice")
    ],
    title="Smart Legal Navigator (Multilingual + Voice)",
    description="Ask legal questions via text or voice in multiple Indian languages. Answers are translated and spoken back."
)

iface.launch()

import gradio as gr    # apting to answer not translating
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
import torch
import tempfile
from gtts import gTTS
import speech_recognition as sr

# Load legal context
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json"
    ]

    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append({
                    "question": entry["question"],
                    "answer": entry["answer"]
                })
    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Load model
device = 0 if torch.cuda.is_available() else -1
print(f"Device set to use {'GPU' if device == 0 else 'CPU'}")
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad", device=device)

# Load data
legal_data = load_legal_data()
corpus = [entry["question"] + " " + entry["answer"] for entry in legal_data]
vectorizer = TfidfVectorizer().fit(corpus)
corpus_vectors = vectorizer.transform(corpus)

# Language code map for TTS
lang_map = {
    "English": "en",
    "Hindi": "hi",
    "Tamil": "ta",
    "Telugu": "te",
    "Bengali": "bn"
}

# Answer logic
def smart_legal_assist(query):
    query_vec = vectorizer.transform([query])
    sims = cosine_similarity(query_vec, corpus_vectors).flatten()
    top_idx = sims.argmax()
    top_entry = legal_data[top_idx]
    context = top_entry["answer"]
    question = query
    answer = qa_model(question=question, context=context)["answer"]
    return answer

# Translate text (mock ‚Äì replace with real translator if needed)
def translate_text(text, src_lang, tgt_lang):
    if src_lang == tgt_lang or not text:
        return text
    return f"[{src_lang}->{tgt_lang}] {text}"  # replace with actual translation logic

# Main handler
def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    if not input_text and audio_input:
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_input) as source:
            audio = recognizer.record(source)
        try:
            input_text = recognizer.recognize_google(audio)
        except sr.UnknownValueError:
            return "Could not understand audio.", None

    if not input_text:
        return "Please enter or say a legal question.", None

    # Translate to English (mock, replace if needed)
    input_english = translate_text(input_text, lang_choice, "English")

    # Get answer
    answer_en = smart_legal_assist(input_english)

    # Translate back
    answer_final = translate_text(answer_en, "English", lang_choice)

    # Convert to speech
    lang_code = lang_map.get(lang_choice, "en")
    tts = gTTS(text=answer_final, lang=lang_code)
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_file.name)

    return answer_final, temp_file.name

# Gradio UI
iface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question (optional)"),
        gr.Audio(type="filepath", label="Or ask your question by voice"),
        gr.Dropdown(["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language", value="English")
    ],
    outputs=[
        gr.Textbox(label="Legal Answer"),
        gr.Audio(label="Spoken Answer")
    ],
    title="üßë‚Äç‚öñÔ∏è Smart Legal Navigator (Multilingual + Voice)",
    description="Ask legal questions via text or voice in Indian languages. Answers are spoken too!"
)

iface.launch()

pip install deep-translator gTTS SpeechRecognition

import gradio as gr   #here translator works  not apting to the answer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from deep_translator import GoogleTranslator
import speech_recognition as sr
from gtts import gTTS
import torch
import tempfile
import json
import os

# Load legal data
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json",
        "/content/reduced_combined_qa (1).json"
    ]
    for path in file_paths:
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for entry in data:
                    context_list.append(entry['answer'])
        else:
            print(f"‚ùå File not found: {path}")
    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Initialize model and vectorizer
device = 0 if torch.cuda.is_available() else -1
print(f"Device set to use {'GPU' if device == 0 else 'CPU'}")
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad", device=device)

legal_context = load_legal_data()
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Core QA function
def smart_legal_assist(user_question):
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

# Translation function
def translate_text(text, target_lang):
    try:
        return GoogleTranslator(source='auto', target=target_lang).translate(text)
    except Exception as e:
        print("Translation error:", e)
        return text

# Main handler
def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    if not input_text and audio_input:
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_input) as source:
            audio = recognizer.record(source)
        try:
            input_text = recognizer.recognize_google(audio)
        except sr.UnknownValueError:
            return "Sorry, I could not understand the audio.", None

    if not input_text:
        return "Please enter or speak a legal question.", None

    # Get English answer
    answer_en = smart_legal_assist(input_text)

    # Translate to selected language
    lang_code_map = {
        "English": "en",
        "Hindi": "hi",
        "Tamil": "ta",
        "Telugu": "te",
        "Bengali": "bn"
    }

    lang_code = lang_code_map.get(lang_choice, "en")
    translated_answer = translate_text(answer_en, lang_code)

    # Convert to speech
    tts = gTTS(text=translated_answer, lang=lang_code)
    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_audio.name)

    return translated_answer, temp_audio.name

# Gradio UI
iface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question (optional)"),
        gr.Audio(type="filepath", label="Or ask your question by voice"),
        gr.Dropdown(["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language")
    ],
    outputs=[
        gr.Textbox(label="Legal Answer"),
        gr.Audio(label="Answer in Voice")
    ],
    title="Smart Legal Navigator (Multilingual + Voice)",
    description="Ask legal questions via text or voice in Indian languages. Get answers translated and spoken back to you."
)

iface.launch()

pip install gradio transformers scikit-learn gtts SpeechRecognition

pip install googletrans==4.0.0-rc1

!pip install gradio

!pip uninstall -y googletrans httpx httpcore
!pip install googletrans==4.0.0-rc1

!pip install gradio

!pip install googletrans==3.0.0

pip install transformers scikit-learn gtts speechrecognition deep-translator gradio

!pip install gradio transformers scikit-learn gtts SpeechRecognition deep-translator pydub

from pydub import AudioSegment  # Add this import at the top

def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    # Convert voice to text if no text input
    if not input_text and audio_input:
        recognizer = sr.Recognizer()

        try:
            # Convert mp3 to wav (speech_recognition doesn't handle mp3 directly)
            sound = AudioSegment.from_file(audio_input, format="mp3")
            wav_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            sound.export(wav_path, format="wav")

            with sr.AudioFile(wav_path) as source:
                audio = recognizer.record(source)
            input_text = recognizer.recognize_google(audio)

        except sr.UnknownValueError:
            return "Sorry, could not understand the audio.", None
        except Exception as e:
            return f"Audio processing error: {str(e)}", None

    if not input_text:
        return "Please enter or speak a legal question.", None

    answer_en = smart_legal_assist(input_text)
    translated_answer = translate_text(answer_en, lang_choice)

    # Generate audio response
    lang_map = {"English": "en", "Hindi": "hi", "Tamil": "ta", "Telugu": "te", "Bengali": "bn"}
    lang_code = lang_map.get(lang_choice, "en")
    tts = gTTS(text=translated_answer, lang=lang_code)
    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_audio.name)

    return translated_answer, temp_audio.name



import gradio as gr   # transltor works and matches with answer (Final)
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
import tempfile
from gtts import gTTS
import speech_recognition as sr
from deep_translator import GoogleTranslator

# Load legal data from JSON files
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/reduced_combined_qa.json"
    ]
    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])

    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Load model and data
legal_context = load_legal_data()
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Translate helper using deep_translator
def translate_text(text, target_lang):
    if not text:
        return ""
    lang_codes = {
        "English": "en", "Hindi": "hi", "Tamil": "ta", "Telugu": "te", "Bengali": "bn"
    }
    tgt = lang_codes.get(target_lang, "en")
    return GoogleTranslator(source='auto', target=tgt).translate(text)

# Core QA logic
def smart_legal_assist(user_question):
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

# Main function for Gradio
def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    # Convert voice to text if no text input
    if not input_text and audio_input:
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_input) as source:
            audio = recognizer.record(source)
        try:
            input_text = recognizer.recognize_google(audio)
        except sr.UnknownValueError:
            return "Sorry, could not understand the audio.", None

    if not input_text:
        return "Please enter or speak a legal question.", None

    answer_en = smart_legal_assist(input_text)
    translated_answer = translate_text(answer_en, lang_choice)

    # Generate audio response
    lang_map = {"English": "en", "Hindi": "hi", "Tamil": "ta", "Telugu": "te", "Bengali": "bn"}
    lang_code = lang_map.get(lang_choice, "en")
    tts = gTTS(text=translated_answer, lang=lang_code)
    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_audio.name)

    return translated_answer, temp_audio.name

# Gradio interface
iface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question (optional)"),
        gr.Audio(type="filepath", label="Or ask your question by voice"),
        gr.Dropdown(["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language")
    ],
    outputs=[
        gr.Textbox(label="Answer"),
        gr.Audio(label="Voice Answer")
    ],
    title="Smart Legal Navigator (Multilingual)",
    description="Ask legal questions via text or voice. Answer comes from the dataset and is translated to your language!"
)

iface.launch()

!pip install pydub
# For Linux/Colab
# Or on Windows/Mac, install ffmpeg and make sure it's in PATH

import gradio as gr   #Final both multingual text and audio conversion
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
import tempfile
from gtts import gTTS
import speech_recognition as sr
from deep_translator import GoogleTranslator
from pydub import AudioSegment  # To convert MP3 to WAV

# Load legal data from JSON files
def load_legal_data():
    context_list = []
    file_paths = [
        "/content/drive/MyDrive/SmartLegalNavigator/constitution_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/crpc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/ipc_qa.json",
        "/content/drive/MyDrive/SmartLegalNavigator/reduced_combined_qa.json"
    ]
    for path in file_paths:
        if not os.path.exists(path):
            print(f"‚ùå File not found: {path}")
            continue
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data:
                context_list.append(entry['answer'])

    if not context_list:
        raise ValueError("üö® No legal data loaded! Check your files.")
    return context_list

# Load model and data
legal_context = load_legal_data()
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
vectorizer = TfidfVectorizer().fit(legal_context)
context_vectors = vectorizer.transform(legal_context)

# Translate helper using deep_translator
def translate_text(text, target_lang):
    if not text:
        return ""
    lang_codes = {
        "English": "en", "Hindi": "hi", "Tamil": "ta", "Telugu": "te", "Bengali": "bn"
    }
    tgt = lang_codes.get(target_lang, "en")
    return GoogleTranslator(source='auto', target=tgt).translate(text)

# Core QA logic
def smart_legal_assist(user_question):
    question_vec = vectorizer.transform([user_question])
    similarities = cosine_similarity(question_vec, context_vectors).flatten()
    top_idx = similarities.argmax()
    top_context = legal_context[top_idx]
    result = qa_model(question=user_question, context=top_context)
    return result['answer']

# Main function for Gradio
def handle_input(text_input, audio_input, lang_choice):
    input_text = text_input

    # Convert voice to text if no text input
    if not input_text and audio_input:
        try:
            # Convert MP3 to WAV
            sound = AudioSegment.from_file(audio_input, format="mp3")
            wav_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            sound.export(wav_path, format="wav")

            # Use speech recognition on WAV
            recognizer = sr.Recognizer()
            with sr.AudioFile(wav_path) as source:
                audio = recognizer.record(source)
            input_text = recognizer.recognize_google(audio)

        except sr.UnknownValueError:
            return "‚ùå Sorry, could not understand the audio.", None
        except Exception as e:
            return f"‚ùå Audio processing error: {str(e)}", None

    if not input_text:
        return "‚ùó Please enter or speak a legal question.", None

    # Get answer
    answer_en = smart_legal_assist(input_text)
    translated_answer = translate_text(answer_en, lang_choice)

    # Generate audio response
    lang_map = {"English": "en", "Hindi": "hi", "Tamil": "ta", "Telugu": "te", "Bengali": "bn"}
    lang_code = lang_map.get(lang_choice, "en")
    tts = gTTS(text=translated_answer, lang=lang_code)
    temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_audio.name)

    return translated_answer, temp_audio.name

# Gradio interface
iface = gr.Interface(
    fn=handle_input,
    inputs=[
        gr.Textbox(label="Type your legal question (optional)"),
        gr.Audio(type="filepath", label="Or ask your question by voice"),
        gr.Dropdown(["English", "Hindi", "Tamil", "Telugu", "Bengali"], label="Select Language")
    ],
    outputs=[
        gr.Textbox(label="Answer"),
        gr.Audio(label="Voice Answer")
    ],
    title="Smart Legal Navigator (Multilingual)",
    description="Ask legal questions via text or voice. Answer comes from the dataset and is translated to your language!"
)

iface.launch()









